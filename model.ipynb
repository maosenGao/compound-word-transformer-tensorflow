{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1bf6d8-e167-469b-bf42-ca217d9d052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import tensorflow_probability as tfp\n",
    "import random\n",
    "from pathlib import Path\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67044b1-5543-4410-926b-d3ba29e25327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "\n",
    "class model:\n",
    "\n",
    "    def getTransformerXL(self, config, log_dir, checkpoint_dir): # get Transformer XL \n",
    "        # [?]XL什么意思？\n",
    "\n",
    "        if log_dir != None:\n",
    "            for key in config.keys():\n",
    "                log_dir += f\"-{key}{config[key]}\"\n",
    "            Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if checkpoint_dir != None:\n",
    "            for key in config.keys():\n",
    "                checkpoint_dir += f\"-{key}{config[key]}\"\n",
    "            Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        mt = TransformerXL(\n",
    "            num_layers=config['num_layers'],\n",
    "            d_model=config['d_model'],\n",
    "            num_heads=config['num_heads'],\n",
    "            dff=config['dff'],\n",
    "            vocab_size=config['vocab_size'],\n",
    "            length=config['length'],\n",
    "            rate=config['dropout_rate'],\n",
    "            rpr=config['rpr']\n",
    "        )\n",
    "\n",
    "        mt.log_dir = log_dir\n",
    "        mt.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        return mt\n",
    "\n",
    "    def getTransformer(self, config, log_dir, checkpoint_dir): # get Transformer\n",
    "\n",
    "        if log_dir != None:\n",
    "            for key in config.keys():\n",
    "                log_dir += f\"-{key}{config[key]}\"\n",
    "            Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if checkpoint_dir != None:\n",
    "            for key in config.keys():\n",
    "                checkpoint_dir += f\"-{key}{config[key]}\"\n",
    "            Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        mt = Transformer(\n",
    "            num_layers=config['num_layers'],\n",
    "            d_model=config['d_model'],\n",
    "            num_heads=config['num_heads'],\n",
    "            dff=config['dff'],\n",
    "            vocab_size=config['vocab_size'],\n",
    "            enc_length=config['enc_length'],\n",
    "            dec_length=config['dec_length'],\n",
    "            rate=config['dropout_rate'],\n",
    "            rpr=config['rpr']\n",
    "        )\n",
    "\n",
    "        mt.log_dir = log_dir\n",
    "        mt.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        return mt\n",
    "\n",
    "    def getLinearTransformerXL(self, config, log_dir=None, checkpoint_dir=None): \n",
    "        # get Linear Transformer XL\n",
    "        # 这个函数用到了\n",
    "        c = config\n",
    "        # if log_dir !=None :\n",
    "        #   for key in c.keys(): log_dir += f\"-{key}{ '|'.join(c[key]) if isinstance(c[key], list) else c[key]}\"\n",
    "        #   Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        def process_dir(dir):\n",
    "            for key in c.keys():\n",
    "                dir += f\"-{key}{ '-'.join([str(i) for i in c[key]]) if isinstance(c[key], list) else c[key]}\"\n",
    "            return dir\n",
    "\n",
    "        if log_dir != None:\n",
    "            log_dir = process_dir(log_dir)\n",
    "            Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if checkpoint_dir != None:\n",
    "            checkpoint_dir = process_dir(checkpoint_dir)\n",
    "            Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        mt = LinearTransformerXL(\n",
    "            vocab_sizes=config['vocab_sizes'],\n",
    "            emb_sizes=config['emb_sizes'],\n",
    "            num_layers=config['num_layers'],\n",
    "            d_model=config['d_model'],\n",
    "            num_heads=config['num_heads'],\n",
    "            dff=config['dff'],\n",
    "            length=config['length'],\n",
    "            rate=config['dropout_rate'],\n",
    "            rpr=config['rpr']\n",
    "        )\n",
    "\n",
    "        mt.log_dir = log_dir\n",
    "        mt.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        return mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db181d7b-7a6e-4633-9065-513c58359795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer): # Multi Head Attention\n",
    "    def __init__(self, d_model, num_heads, maximum_position_encoding, rpr=True):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.max_seq = maximum_position_encoding\n",
    "        self.rpr = rpr\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def build(self, x):\n",
    "        self.E = self.add_weight('emb', shape=[self.max_seq, int(self.depth)])\n",
    "        # self.E_decoder = self.add_weight('emb', shape=[self.max_seq, int(self.depth)])\n",
    "\n",
    "    def _get_left_embedding(self, len_q):\n",
    "        # starting_point = max(0,self.max_seq-len_q)\n",
    "\n",
    "        s = self.max_seq-len_q\n",
    "        if s > 0:\n",
    "            starting_point = s\n",
    "        else:\n",
    "            starting_point = 0\n",
    "\n",
    "        e = self.E[starting_point:, :]\n",
    "        return e\n",
    "\n",
    "    @staticmethod\n",
    "    def _qe_masking(qe):\n",
    "        mask = tf.sequence_mask(\n",
    "            tf.range(tf.shape(qe)[-1] - 1, tf.shape(qe)[-1] - tf.shape(qe)[-2] - 1, -1), tf.shape(qe)[-1])\n",
    "\n",
    "        mask = tf.logical_not(mask)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "        return mask * qe\n",
    "\n",
    "    def _skewing(self, tensor: tf.Tensor):\n",
    "        padded = tf.pad(tensor, [[0, 0], [0, 0], [0, 0], [1, 0]])\n",
    "        reshaped = tf.reshape(\n",
    "            padded, shape=[-1, tf.shape(padded)[1], tf.shape(padded)[-1], tf.shape(padded)[-2]])\n",
    "        Srel = reshaped[:, :, 1:, :]\n",
    "        if self.len_k > self.len_q:\n",
    "            Srel = tf.pad(Srel, [[0, 0], [0, 0], [0, 0],\n",
    "                          [0, self.len_k-self.len_q]])\n",
    "        elif self.len_k < self.len_q:\n",
    "            Srel = Srel[:, :, :, :self.len_k]\n",
    "        return Srel\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        \"\"\"Calculate the attention weights.\n",
    "        q, k, v must have matching leading dimensions.\n",
    "        k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "        The mask has different shapes depending on its type(padding or look ahead)\n",
    "        but it must be broadcastable for addition.\n",
    "\n",
    "        Args:\n",
    "          q: query shape == (..., seq_len_q, depth)\n",
    "          k: key shape == (..., seq_len_k, depth)\n",
    "          v: value shape == (..., seq_len_v, depth_v)\n",
    "          mask: Float tensor with shape broadcastable\n",
    "                to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "          output, attention_weights\n",
    "        \"\"\"\n",
    "\n",
    "        # (..., seq_len_q, seq_len_k)\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "        if self.rpr:\n",
    "            E = self._get_left_embedding(tf.shape(q)[2])\n",
    "            QE = tf.einsum('bhld,md->bhlm', q, E)\n",
    "            QE = self._qe_masking(QE)\n",
    "            Srel = self._skewing(QE)\n",
    "            matmul_qk = matmul_qk + Srel\n",
    "\n",
    "        # scale matmul_qk\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "        # add up to 1.\n",
    "        attention_weights = tf.nn.softmax(\n",
    "            scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len_q, depth)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        # (batch_size, num_heads, seq_len_k, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        # (batch_size, num_heads, seq_len_v, depth)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        self.len_q = tf.shape(q)[2]\n",
    "        self.len_k = tf.shape(k)[2]\n",
    "        self.len_v = tf.shape(v)[2]\n",
    "        # print('''''',self.len_q, self.len_k, self.len_v)\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        # (batch_size, seq_len_q, num_heads, depth)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bce0cc-be8d-455c-b234-4127037bf6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, maximum_position_encoding, rate=0.1, rpr=False):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "            d_model, num_heads, maximum_position_encoding, rpr)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        attn_output, attn_weights = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01bc798-d23e-4cfb-995e-dbae4de0b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):  # 编码\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1, rpr=False):\n",
    "\n",
    "        print('num_layers', num_layers)\n",
    "        print('d_model', d_model)\n",
    "        print('num_heads', num_heads)\n",
    "        print('dff', dff)\n",
    "        print('input_vocab_size', input_vocab_size)\n",
    "        print('maximum_position_encoding', maximum_position_encoding)\n",
    "        # maximum_position_encoding\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                                self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, maximum_position_encoding, rate, rpr)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, attn_weights = self.enc_layers[i](x, training, mask)\n",
    "            attention_weights[f'encoder_layer{i+1}'] = attn_weights\n",
    "        return x, attention_weights  # x = (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b9a6d1-461b-47c0-a553-5656fdfb700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, maximum_position_encoding, rate=0.1, rpr=False):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(\n",
    "            d_model, num_heads, maximum_position_encoding, rpr)\n",
    "        self.mha2 = MultiHeadAttention(\n",
    "            d_model, num_heads, maximum_position_encoding, rpr)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "             look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6605410-d585-41c7-83e4-8d7f3902f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer): # 解码\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1, rpr=False):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(\n",
    "            maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, maximum_position_encoding, rate, rpr)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "             look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7098c-c53b-41c0-af37-1e022eba666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                 enc_length, dec_length, rate=0.1, rpr=False):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                               vocab_size, enc_length, rate, rpr)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               vocab_size, dec_length, rate, rpr)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.enc_length = enc_length\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp, tar = inputs\n",
    "\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(\n",
    "            inp, tar)\n",
    "\n",
    "        # (batch_size, inp_seq_len, d_model)\n",
    "        enc_output, _ = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def eval(self, eval_seq):\n",
    "\n",
    "        enc_inp = eval_seq[:, :self.enc_length]\n",
    "        dec_inp = eval_seq[:, :-1]\n",
    "        tar_real = eval_seq[:, 1:]\n",
    "        predictions, _ = self([enc_inp, dec_inp])\n",
    "        loss = self.loss_function(tar_real, predictions)\n",
    "        acc = self.accuracy_function(tar_real, predictions)\n",
    "        return loss, acc\n",
    "\n",
    "    def generate(self, inp, length=1024, random_seed=0.7):\n",
    "        seed, gen = inp\n",
    "        inp_len = len(gen[0])\n",
    "        with tqdm(total=length - inp_len) as bar:\n",
    "            bar.updata(inp_len)\n",
    "            while len(gen[0]) < length:\n",
    "\n",
    "                y, _ = self([seed, gen], False)\n",
    "                u = random.uniform(0, 1)\n",
    "\n",
    "                if u > random_seed:\n",
    "                    tf.argmax(y[:, -1], -1)\n",
    "                    y = tf.argmax(y[:, -1], -1)\n",
    "                    y = tf.cast(y, tf.int64)\n",
    "                    gen = tf.concat([gen, tf.expand_dims(y, -1)], -1)\n",
    "\n",
    "                else:\n",
    "                    probs = tf.nn.softmax(y[:, -1])\n",
    "                    pdf = tfp.distributions.Categorical(probs=probs)\n",
    "                    y = pdf.sample(1)\n",
    "                    y = tf.transpose(y, (1, 0))\n",
    "                    y = tf.cast(y, tf.int64)\n",
    "                    gen = tf.concat([gen, y], -1)\n",
    "\n",
    "                bar.update(1)\n",
    "\n",
    "        return gen\n",
    "\n",
    "    def train_setup(self, loss_function, accuracy_function, optimizer, mirrored_strategy=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy_function = accuracy_function\n",
    "        self.loss_function = loss_function\n",
    "        self.mirrored_strategy = mirrored_strategy\n",
    "\n",
    "    train_step_signature = [\n",
    "        tf.TensorSpec(shape=(None, None), dtype=tf.int64)\n",
    "    ]\n",
    "\n",
    "    @tf.function(input_signature=train_step_signature)\n",
    "    def train_step(self, seq):\n",
    "        enc_inp = seq[:, :self.enc_length]\n",
    "        dec_inp = seq[:, :-1]\n",
    "        tar_real = seq[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = self([enc_inp, dec_inp], training=True)\n",
    "            loss = self.loss_function(tar_real, predictions)\n",
    "            accuracy = self.accuracy_function(tar_real, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    # @tf.function(input_signature=train_step_signature)\n",
    "    @tf.function\n",
    "    def distributed_train_step(self, seq):\n",
    "\n",
    "        per_replica_losses, per_replica_accuracy = self.mirrored_strategy.run(\n",
    "            self.train_step, args=(seq,))\n",
    "        losses = self.mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                                               axis=None)\n",
    "        accuracy = self.mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_accuracy,\n",
    "                                                 axis=None)\n",
    "        return losses, accuracy\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        # Encoder padding mask\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cee23b-b3d1-47db-a763-8be5ca3b778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXL(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                 length, rate=0.1, rpr=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                               vocab_size, length, rate, rpr)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp = inputs\n",
    "\n",
    "        _, look_ahead_mask, _ = self.create_masks(inp, inp)\n",
    "\n",
    "        # (batch_size, inp_seq_len, d_model)\n",
    "        out, attention_weights = self.encoder(inp, training, look_ahead_mask)\n",
    "\n",
    "        # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        final_output = self.final_layer(out)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def eval(self, eval_seq):\n",
    "\n",
    "        eval_inp = eval_seq[:, :-1]\n",
    "        eval_real = eval_seq[:, 1:]\n",
    "        predictions, _ = self(eval_inp)\n",
    "        eval_loss = self.loss_function(eval_real, predictions)\n",
    "        eval_acc = self.accuracy_function(eval_real, predictions)\n",
    "        return eval_loss, eval_acc\n",
    "\n",
    "    def generate(self, inp, length=1024, random_seed=0.7):\n",
    "        gen = inp\n",
    "        inp_len = len(gen[0])\n",
    "        with tqdm(total=length) as bar:\n",
    "            bar.update(inp_len)\n",
    "            while len(gen[0]) < length:\n",
    "\n",
    "                y, _ = self(gen, False)\n",
    "\n",
    "                u = random.uniform(0, 1)\n",
    "                if u > random_seed:\n",
    "                    y = tf.argmax(y[:, -1], -1)\n",
    "                    y = tf.cast(y, tf.int64)\n",
    "                    gen = tf.concat([gen, tf.expand_dims(y, -1)], -1)\n",
    "\n",
    "                else:\n",
    "                    probs = tf.nn.softmax(y[:, -1])\n",
    "                    pdf = tfp.distributions.Categorical(probs=probs)\n",
    "                    y = pdf.sample(1)\n",
    "                    y = tf.transpose(y, (1, 0))\n",
    "                    y = tf.cast(y, tf.int64)\n",
    "                    gen = tf.concat([gen, y], -1)\n",
    "\n",
    "                bar.update(1)\n",
    "\n",
    "        return gen\n",
    "\n",
    "    def train_setup(self, loss_function, accuracy_function, optimizer, mirrored_strategy=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy_function = accuracy_function\n",
    "        self.loss_function = loss_function\n",
    "        self.mirrored_strategy = mirrored_strategy\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, seq):\n",
    "        dec_inp = seq[:, :-1]\n",
    "        tar_real = seq[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = self(dec_inp, training=True)\n",
    "            loss = self.loss_function(tar_real, predictions)\n",
    "            accuracy = self.accuracy_function(tar_real, predictions)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.trainable_variables))\n",
    "        return loss, accuracy\n",
    "\n",
    "    # @tf.function(input_signature=train_step_signature)\n",
    "    @tf.function\n",
    "    def distributed_train_step(self, seq):\n",
    "\n",
    "        with self.mirrored_strategy.scope():\n",
    "            per_replica_losses, per_replica_accuracy = self.mirrored_strategy.run(\n",
    "                self.train_step, args=(seq,))\n",
    "            losses = self.mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                                                   axis=None)\n",
    "            accuracy = self.mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_accuracy,\n",
    "                                                     axis=None)\n",
    "        return losses, accuracy\n",
    "    # def train(self):\n",
    "    # def generate(self, inp, length = 1024):\n",
    "    #   # self.\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        # Encoder padding mask\n",
    "        enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar)\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf97e81-ef20-486e-9d97-531c9f049934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTransformerXL(tf.keras.Model):\n",
    "    def __init__(self, vocab_sizes, emb_sizes, num_layers, d_model, num_heads, dff,\n",
    "                 length, rate=0.1, rpr=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_sizes = vocab_sizes\n",
    "        self.emb_sizes = emb_sizes\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, length, rate, rpr)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.last_dim = len(vocab_sizes)\n",
    "\n",
    "        # process inp\n",
    "        self.inp_embeddings = [tf.keras.layers.Embedding(self.vocab_sizes[i], self.emb_sizes[i])\n",
    "                               for i in range(self.last_dim)]\n",
    "\n",
    "        self.inp_dense = tf.keras.layers.Dense(self.d_model)\n",
    "        self.pos_encoding = positional_encoding(length, self.d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "        # process out\n",
    "        self.out_denses = [tf.keras.layers.Dense(self.vocab_sizes[i])\n",
    "                           for i in range(self.last_dim)]\n",
    "    def call(self, inputs, training, tar=None, for_family_type = False, family_type=None, h=None):\n",
    "        \n",
    "        if h !=None: # reasoning by family type\n",
    "        \n",
    "          x_ = tf.concat([h, self.inp_embeddings[0](family_type)], -1)\n",
    "          out_group = [self.out_denses[i+1](x_) for i in range(self.last_dim - 1)]\n",
    "          return out_group, x_\n",
    "        \n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        inp_group = [self.inp_embeddings[i](inputs[..., i])\n",
    "                     for i in range(self.last_dim)]\n",
    "\n",
    "        all_embedding = tf.concat(\n",
    "            inp_group, -1)\n",
    "        x = self.inp_dense(all_embedding)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        mask = create_look_ahead_mask(seq_len)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, attn_weights = self.enc_layers[i](x, training, mask)\n",
    "            attention_weights[f'encoder_layer{i+1}'] = attn_weights\n",
    "\n",
    "        if for_family_type:\n",
    "          return self.out_denses[0](x), x\n",
    "\n",
    "        if training:\n",
    "          x_ = tf.concat([x, self.inp_embeddings[0](tar[..., 0])], -1)\n",
    "          out_group = [self.out_denses[i](x) if i == 0 else self.out_denses[i](x_)\n",
    "                           for i in range(self.last_dim)]\n",
    "          return out_group, x_\n",
    "\n",
    "    def eval(self, eval_seq):\n",
    "        eval_seq = np.array(eval_seq)\n",
    "        inp = eval_seq[:, :-1]\n",
    "        tar = eval_seq[:, 1:]\n",
    "        predictions, _ = self(inp, True, tar)\n",
    "        dim = inp.shape[-1]\n",
    "        losses = [self.loss_function(tar[..., i], predictions[i])\n",
    "                  for i in range(dim)]\n",
    "        accuracy = [self.accuracy_function(\n",
    "            tar[..., i], predictions[i]) for i in range(dim)]\n",
    "\n",
    "        return losses, accuracy\n",
    "\n",
    "    def test(self, ds, batch_size=20):\n",
    "        \n",
    "        m_loss = tf.keras.metrics.Mean(name=f'test_loss')\n",
    "        m_acc = tf.keras.metrics.Mean(name=f'test_acc')\n",
    "        m_loss.reset_states()\n",
    "        m_acc.reset_states()\n",
    "        total = math.ceil(ds.total_seq / batch_size)\n",
    "        random.seed(1)\n",
    "        idxs = random.sample(\n",
    "            range(ds.total_seq), ds.total_seq)\n",
    "        idxs = idxs[: 1000]\n",
    "        with tqdm(total=total) as bar:\n",
    "            for batch_idxs in batch(idxs, batch_size):\n",
    "                seqs = ds.get_seqs(batch_idxs)\n",
    "                e_losses, e_acc = self.eval(seqs)\n",
    "                m_loss(np.sum([l.numpy() for l in e_losses]) / len(e_losses)) # 均值\n",
    "                m_acc(np.sum(e_acc) / len(e_losses))\n",
    "                bar.set_description('Test>>>>>>>>>>')\n",
    "                bar.update(1)\n",
    "        return m_loss.result().numpy(), m_acc.result().numpy()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, seq):\n",
    "\n",
    "        dec_inp = seq[:, :-1]\n",
    "        tar_real = seq[:, 1:]\n",
    "        dim = dec_inp.shape[-1]\n",
    "\n",
    "        # print(self.trainable_variables)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = self(dec_inp, True, tar_real)\n",
    "            losses = [self.loss_function(\n",
    "                tar_real[..., i], predictions[i]) for i in range(dim)]\n",
    "            accuracy = [self.accuracy_function(\n",
    "                tar_real[..., i], predictions[i]) for i in range(dim)]\n",
    "\n",
    "        gradients = tape.gradient(losses, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.trainable_variables))\n",
    "        return losses, accuracy\n",
    "\n",
    "    def train_setup(self, loss_function, accuracy_function, optimizer, mirrored_strategy=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy_function = accuracy_function\n",
    "        self.loss_function = loss_function\n",
    "        self.mirrored_strategy = mirrored_strategy\n",
    "        \n",
    "    def generate(self, inp, length, temperature, nucleus_p,  if_end = False):\n",
    "        gen = inp\n",
    "        inp_len = len(gen[0])\n",
    "        tp = temperature\n",
    "        dim = gen.shape[-1]\n",
    "        eos_tokens = [i-1 for i in self.vocab_sizes]\n",
    "        with tqdm(total=length) as bar:\n",
    "            bar.update(inp_len)\n",
    "\n",
    "            while len(gen[0]) < length:\n",
    "                gen_ = gen[:,-1024:]\n",
    "                y_ft, h = self(gen_, False, for_family_type = True) # get family type\n",
    "                \n",
    "                ft_logits = y_ft[:, -1] #(batch, vocab_size)\n",
    "                \n",
    "                sampling_ft = sampling(ft_logits, t=tp[0], p=nucleus_p[0])\n",
    "                \n",
    "                gen_ft = gen_[...,0][:, 1:] #(batch_size, seq_len-1)\n",
    "                \n",
    "                gen_ft = tf.concat([gen_ft, tf.expand_dims(sampling_ft, 1)],1) #(batch_size, seq_len)\n",
    "                \n",
    "                y,_ = self(gen_, False, family_type = gen_ft, h=h) # get [Bar/position Pitch Velocity Duration Tempo]\n",
    "                \n",
    "                r = np.array([sampling_ft]) # (1 batch_size )\n",
    "                \n",
    "                for i in range(dim-1):\n",
    "                    \n",
    "                    logits = np.array(y[i][:, -1])  \n",
    "                    if if_end ==False: \n",
    "                           \n",
    "                        logits[...,-1] -= 1e5 # mask eos token\n",
    "                        \n",
    "                    if i==6: # Tempo\n",
    "                          \n",
    "                          ones = np.ones(len(logits)) # (batch)  \n",
    "                          mask = tf.cast(np.equal(r[0], ones*2), tf.float32) # if Family_Metric --> mask ignore\n",
    "                          mask2 = tf.cast(np.not_equal(r[1], ones*1), tf.float32) # if Family_Metric --> mask ignore\n",
    "                          logits[...,1] -= mask * mask2 * 1e5\n",
    "\n",
    "                    if i==0: # position\n",
    "                        \n",
    "                          ones = np.ones(len(logits)) # (batch)                \n",
    "                          last_positoin = np.squeeze(gen[:,-1,1]) # (batch)                            \n",
    "                          mask = tf.cast(np.equal(last_positoin, ones*1), tf.float32) \n",
    "                          logits[...,1] -= mask * 1e5 # if last position is bar, mask this bar\n",
    "                            \n",
    "                    t = tp[i+1]\n",
    "                    p = sampling(logits, t=t, p=nucleus_p[i+1])\n",
    "                    r = np.append(r, [p], 0) # (last_dim, batch_size)\n",
    "                if if_end:\n",
    "                    squeeze_r = np.squeeze(r)\n",
    "                    if np.any(np.equal(eos_tokens, squeeze_r) == True): break \n",
    "                gen = tf.concat([gen, tf.expand_dims(tf.transpose(r), 1)], 1)\n",
    "\n",
    "                bar.update(1)\n",
    "\n",
    "        return gen\n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fbe51-4854-456d-8376-50503af3fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_process_json:\n",
    "    def __init__(self, dir_path):\n",
    "        self.dir_path = dir_path\n",
    "        Path(dir_path).touch(exist_ok=True)\n",
    "\n",
    "    def get(self):\n",
    "        f = open(self.dir_path, 'r')\n",
    "        content = f.read()\n",
    "        if content == '':\n",
    "            return {\"step\": 0, \"best_acc\": 0.0, \"best_loss\": 10000.0}\n",
    "        else:\n",
    "            config = json.loads(content)\n",
    "            for key in ['best_acc', 'best_loss']:\n",
    "                config[key] = float(config[key])\n",
    "            return config\n",
    "\n",
    "    def set(self, step, best_acc, best_loss):\n",
    "        config = self.get()\n",
    "        config['step'] = step\n",
    "        config['best_acc'] = str(best_acc)\n",
    "        config['best_loss'] = str(best_loss)\n",
    "\n",
    "        with open(self.dir_path, 'w') as f:\n",
    "            json.dump(config, f)\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
